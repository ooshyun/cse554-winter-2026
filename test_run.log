========================================================================
CSE 554 ASSIGNMENT 1 - MASTER TEST SUITE
========================================================================

This script will run all implementations and tests:
  - Section 1: SiLU (PyTorch, Triton, CUDA)
  - Section 2: RMS Norm (Matrix and Vector)
  - Section 3: Host-GPU Memory Copy

========================================================================

Test started at: 2026. 01. 21. (수) 02:21:42 KST
Log file: claudedocs/test_results_20260121_022142.log


========================================================================
SECTION 1: SiLU
========================================================================

========================================================================
Running: Section 1 - PyTorch/Triton
========================================================================
========================================================================
CSE 554 Assignment 1 - Section 1: SiLU (Python/Triton)
========================================================================

✓ CUDA is available

========================================================================
Running PyTorch SiLU Implementation...
========================================================================
ERROR:2026-01-21 02:21:44 1950389:1950389 output_json.cpp:117] Failed to open 'profiling_results/torch_silu.json' : No such file or directory
ERROR:2026-01-21 02:21:44 1950389:1950389 output_json.cpp:559] Failed to write to log file!
/home/seunghyunoh/workspace/research/uw-cs554/srcs/python/silu/silu_torch.py:140: FutureWarning: `cuda_time` is deprecated, please use `device_time` instead.
  elif hasattr(evt, 'cuda_time'):
/home/seunghyunoh/workspace/research/uw-cs554/srcs/python/silu/silu_torch.py:141: FutureWarning: `cuda_time` is deprecated, please use `device_time` instead.
  print(f"  - {evt.key}: {evt.cuda_time / 1000:.4f} ms")
CSE 554 Assignment 1 - Section 1: SiLU (PyTorch Implementation)
================================================================================
CUDA Device: NVIDIA GeForce RTX 4070 Ti SUPER
Using device: cuda

Torch Profiler results saved to: profiling_results/torch_silu.json

================================================================================
TORCH PROFILER SUMMARY
================================================================================
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                              aten::mul         0.10%     585.988us         0.18%       1.015ms       5.075us     217.997ms        38.52%     217.997ms       1.090ms           0 b           0 b      50.00 Gb      50.00 Gb           200  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     130.661ms        23.09%     130.661ms       1.307ms           0 b           0 b           0 b           0 b           100  
                                              aten::add         0.05%     307.284us         0.08%     473.431us       4.734us      87.421ms        15.45%      87.421ms     874.205us           0 b           0 b      25.00 Gb      25.00 Gb           100  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      87.421ms        15.45%      87.421ms     874.205us           0 b           0 b           0 b           0 b           100  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      87.336ms        15.43%      87.336ms     873.359us           0 b           0 b           0 b           0 b           100  
                                              aten::exp         0.05%     269.784us         0.08%     432.373us       4.324us      87.153ms        15.40%      87.153ms     871.526us           0 b           0 b      25.00 Gb      25.00 Gb           100  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      87.153ms        15.40%      87.153ms     871.526us           0 b           0 b           0 b           0 b           100  
                                       aten::reciprocal         0.05%     270.982us         0.08%     449.127us       4.491us      87.123ms        15.40%      87.123ms     871.230us           0 b           0 b      25.00 Gb      25.00 Gb           100  
void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      87.123ms        15.40%      87.123ms     871.230us           0 b           0 b           0 b           0 b           100  
                                              aten::neg         0.23%       1.288ms         0.26%       1.465ms      14.654us      86.179ms        15.23%      86.179ms     861.792us           0 b           0 b      25.00 Gb      25.00 Gb           100  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 566.257ms
Self CUDA time total: 565.873ms


================================================================================
PERFORMANCE METRICS
================================================================================
Matrix shape: (8192, 8192)
Average execution time: 5.6615 ms
Minimum memory accesses: 512.00 MB
Bandwidth utilization: 88.32 GB/s
================================================================================

Correctness check:
Input: [[ 1. -1.]
 [ 0.  2.]]
Output: [[ 0.73105854 -0.26894143]
 [ 0.          1.761594  ]]
Expected (nn.functional.silu): [[ 0.73105854 -0.26894143]
 [ 0.          1.761594  ]]
Max difference: 0.00e+00

================================================================================
KERNEL ANALYSIS
================================================================================
Kernels launched during SiLU execution:
  - void at::native::vectorized_elementwise_kernel<4, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::neg_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>): 0.8673 ms
  - void at::native::vectorized_elementwise_kernel<4, at::native::exp_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::exp_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>): 0.8710 ms
  - void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<float>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<float>, at::detail::Array<char*, 2>): 0.8737 ms
  - void at::native::vectorized_elementwise_kernel<4, at::native::reciprocal_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::reciprocal_kernel_cuda(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>): 0.8719 ms
  - void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2>): 0.8739 ms
  - void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>): 1.3038 ms
================================================================================

✓ Profiling complete!
  - Torch Profiler output: profiling_results/torch_silu.json
  - Run Nsight Systems: nsys profile -o profiling_results/torch_silu python3 srcs/python/silu/silu_torch.py

========================================================================
Running Triton SiLU Kernel...
========================================================================
CSE 554 Assignment 1 - Section 1: SiLU (Triton Kernel)
================================================================================
CUDA Device: NVIDIA GeForce RTX 4070 Ti SUPER

Testing correctness...
Input: [[ 1. -1.]
 [ 0.  2.]]
Triton Output: [[ 0.7310586  -0.26894143]
 [ 0.          1.7615942 ]]
Expected: [[ 0.73105854 -0.26894143]
 [ 0.          1.761594  ]]
Max difference: 1.19e-07

Benchmarking Triton kernel...

================================================================================
PERFORMANCE METRICS
================================================================================
Average execution time: 0.8726 ms
Minimum memory accesses: 512.00 MB
Bandwidth utilization: 572.97 GB/s
================================================================================

✓ Triton kernel test complete!

========================================================================
Running Triton SiLU Tests...
========================================================================
CSE 554 Assignment 1 - SiLU Triton Kernel Test Suite
================================================================================
CUDA Device: NVIDIA GeForce RTX 4070 Ti SUPER

Test case 1: max difference = 2.38e-07
Test case 2: max difference = 7.15e-07
Test case 3: max difference = 5.96e-08
Test case 4: max difference = 9.54e-07
✓ All correctness tests passed!

✓ Shape (100,): passed
✓ Shape (1000,): passed
✓ Shape (1024, 1024): passed
✓ Shape (8192, 8192): passed
✓ Shape (100, 200, 300): passed
✓ All shape tests passed!

✓ Edge case 'zeros': max difference = 0.00e+00
✓ Edge case 'ones': max difference = 5.96e-08
✓ Edge case 'large_positive': max difference = 0.00e+00
✓ Edge case 'large_negative': max difference = 0.00e+00
✓ Edge case 'mixed': max difference (finite) = 5.96e-08

Block size 128: max difference = 7.15e-07
Block size 256: max difference = 7.15e-07
Block size 512: max difference = 7.15e-07
Block size 1024: max difference = 7.15e-07
✓ All block size tests passed!


================================================================================
PERFORMANCE BENCHMARKING
================================================================================

Shape: (1024, 1024)
  Execution time: 0.0082 ms
  Bandwidth: 950.66 GB/s

Shape: (4096, 4096)
  Execution time: 0.2184 ms
  Bandwidth: 572.25 GB/s

Shape: (8192, 8192)
  Execution time: 0.8726 ms
  Bandwidth: 572.98 GB/s

================================================================================
✓ ALL TESTS PASSED!
================================================================================

========================================================================
Section 1 (Python/Triton) Complete!
========================================================================

Generated files:
  - profiling_results/torch_silu.json (Torch Profiler)

To profile with Nsight Systems, run:
  nsys profile -o profiling_results/torch_silu python3 srcs/python/silu/silu_torch.py

✓ Section 1 - PyTorch/Triton: PASSED
